# MyoSkeleton MotionGPT Stage 2: Motion-Language Pretraining with Qwen3-0.6B
# Pretrain Qwen3-0.6B with MyoSkeleton motion tokens

NAME: myoskeleton_qwen_stage2_pretrain
DEBUG: false
ACCELERATOR: 'gpu'
DEVICE: [0]

# Model configuration
model:
  target: mGPT.models.mgpt.MotionGPT
  params:
    stage: lm_pretrain
    debug: ${DEBUG}
    codebook_size: 512
    condition: text
    task: t2m
    
    # Frozen MyoSkeleton VQ-VAE (from Stage 1)
    motion_vae:
      target: mGPT.archs.mgpt_vae.MyoSkeletonVQVAE
      params:
        nfeats: 84  # 28 simplified joints * 3
        quantizer: 'ema_reset'
        code_dim: 512
        nb_code: 512
        mu: 0.99
        down_t: 2
        stride_t: 2
        width: 512
        depth: 3
        dilation_growth_rate: 3
        activation: 'relu'
        norm: None
    
    # Qwen3-0.6B Language Model with MyoSkeleton tokens
    lm:
      target: mGPT.archs.mgpt_lm_qwen.QwenMLM
      params:
        model_path: "Qwen/Qwen3-0.6B"
        stage: lm_pretrain
        motion_codebook_size: 512
        framerate: 20
        down_t: 2
        predict_ratio: 0.2
        inbetween_ratio: 0.25
        max_length: 512  # Longer context for Qwen3
        quota_ratio: 0.5
        enable_thinking: true  # Enable Qwen3's thinking capabilities
        new_token_type: "insert"

# Data configuration
DATASET:
  target: mGPT.data.myoskeleton_dataset.MyoSkeletonDataModule
  params:
    data_root: ./datasets/myoskeleton_h5/
    batch_size: 32  # Smaller batch for 0.6B model
    num_workers: 8
    use_simplified_joints: true
    min_motion_len: 40
    max_motion_len: 196
    unit_length: 4
    fps: 20
    # Template instructions for pretraining
    TASK_PATH: ./prepare/instructions/template_pretrain.json

# Training configuration
TRAIN:
  STAGE: lm_pretrain
  NUM_EPOCHS: 2000
  RESUME: ''
  PRETRAINED_VAE: ./checkpoints/myoskeleton_stage1/latest.ckpt  # Load Stage 1 VQ-VAE
  OPTIM:
    TYPE: AdamW
    LR: 5e-5  # Lower LR for smaller model
    WEIGHT_DECAY: 1e-4
    BETAS: [0.9, 0.999]
  SCHEDULER:
    TYPE: cosine
    WARMUP_EPOCHS: 100

# Loss configuration
LOSS:
  TYPE: mGPT
  LAMBDA_CLS: 1.0  # Language model loss weight

# Logging and checkpoints
LOGGER:
  SAVER:
    LOG_EVERY_STEPS: 100
    VAL_EVERY_STEPS: 500
    SAVE_EVERY_STEPS: 2000

# Evaluation
TEST:
  CHECKPOINTS: ./checkpoints/myoskeleton_qwen_stage2/
  DATASETS: ['MyoSkeletonDataset'] 